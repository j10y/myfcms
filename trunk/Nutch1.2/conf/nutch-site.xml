<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
		<name>http.agent.name</name>
		<value>zuso-robot</value>
	</property>
	<property>
		<name>searcher.dir</name>
		<value>D:\workspace\Nutch1.2\crawl</value>
	</property>

	<property>
		<name>searcher.summary.length</name>
		<value>50</value>
		<description>
			The total number of terms to display in a hit summary.
		</description>
	</property>

	<property>
		<name>http.agent.version</name>
		<value>1.1</value>
		<description>
			A version string to advertise in the User-Agent header.
		</description>
	</property>

	<property>
		<name>http.timeout</name>
		<value>200000</value>
		<description>
			The default network timeout, in milliseconds.
		</description>
	</property>

	<property>
		<name>http.redirect.max</name>
		<value>2</value>
		<description>
			The maximum number of redirects the fetcher will follow when
			trying to fetch a page. If set to negative or 0, fetcher
			won't immediately follow redirected URLs, instead it will
			record them for later fetching.
		</description>
	</property>

	<property>
		<name>http.content.limit</name>
		<value>131072</value>
		<description>
			The length limit for downloaded content using the http
			protocol, in bytes. If this value is nonnegative (>=0),
			content longer than it will be truncated; otherwise, no
			truncation at all. Do not confuse this setting with the
			file.content.limit setting.
		</description>
	</property>

	<property>
		<name>plugin.includes</name>
		<value>
			analysis-(zh)|protocol-http|urlfilter-regex|parse-(text|html|js|tika)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)
		</value>
		<description>
			Regular expression naming plugin directory names to include.
			Any plugin not matching this expression is excluded. In any
			case you need at least include the nutch-extensionpoints
			plugin. By default Nutch includes crawling just HTML and
			plain text via HTTP, and basic indexing and search plugins.
			In order to use HTTPS please enable protocol-httpclient, but
			be aware of possible intermittent problems with the
			underlying commons-httpclient library. Nutch now also
			includes integration with Tika to leverage Tika's parsing
			capabilities for multiple content types. The existing Nutch
			parser implementations will likely be phased out in the next
			release or so, as such, it is a good idea to begin migrating
			away from anything not provided by parse-tika.
		</description>
	</property>

	<property>
		<name>parser.character.encoding.default</name>
		<value>utf-8</value>
		<description>
			The character encoding to fall back to when no other
			information is available
		</description>
	</property>

</configuration>
